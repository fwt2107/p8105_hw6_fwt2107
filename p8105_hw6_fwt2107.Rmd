---
title: "p8105_hw6_fwt2107"
author: "Felix Tran"
date: "November 17, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(modelr)
library(mgcv)
library(purrr)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_bw() + theme(legend.position = "right"))
```

# Problem 1

### Data cleaning

1. Read in the dataset from the Washington Post's github page

2. Create a **city_state** variable to combine each homicide's city and state

3. Remove all homicides which were recorded in Dallas, TX; Phoenix, AZ; Kansas
City, MO; and Tulsa, AL

4. Transform **victim_age** into a numeric variable

5. Transform **victim_race** into a binary variable (white vs. non-white)

6. Create a **solved_binary** binary variable for the outcome of each homicide
(1 (solved) vs. 0 (unsolved))
```{r}
data_url <- RCurl::getURL('https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv')

homicide_df <- readr::read_csv(data_url) %>% 
  mutate(city_state = stringr::str_c(city, ', ',state)) %>% 
  filter(!(city_state %in% c('Dallas, TX', 'Phoenix, AZ', 'Kansas City, MO', 
                             'Tulsa, AL'))) %>% 
  mutate(victim_age = as.numeric(victim_age),
         victim_race = ifelse(victim_race == "White", "white", "non-white"),
         solved_binary = ifelse(disposition == "Closed by arrest", 1, 0))
```

### Fitting a logistic regression model for resolved vs. unresolved homicides
### in Baltimore, MD

1. Use the *glm()* function to run a logistic regression modeling the logit
of the probability of a murder being resolved on victim age, sex, and race
in Baltimore, MD.

2. Use *broom::tidy()* to clean the output and compute the OR's and 95% CI for
the estimates
```{r}
baltimore_log <- glm(solved_binary ~ victim_age + victim_sex + victim_race,
                     data = homicide_df, family = binomial())

baltimore_log %>% 
  broom::tidy(.) %>% 
  mutate(OR = exp(estimate),
         CI_lower_limit = exp(estimate - 1.96*std.error),
         CI_upper_limit = exp(estimate + 1.96*std.error)) %>% 
  select(term, estimate, OR, CI_lower_limit, CI_upper_limit)
```

After adjusting for victim age and sex, homicides in Baltimore, MD in which the 
victim was white had approximately 1.78 times the odds of being resolved 
compared to homicides in which the victim was non-white. We are 95% confident
that Baltimore homicides involving white victims had between 1.68 to 1.89 times
the odds of being resolved compared to homicides involving non-white victims.

### Fitting a logistic regression model for resolved vs. unresolved homicides
### in all cities listed in the dataset
1. Wrote *logistic_regression()* function to output a tidied logistic regression
results from the *glm()* function. Logistic regression was run to describe
the logit transformation of the probability of a homicide being resolved as a
function of the victim's race, age, and sex.

2. Ran *logistic_regression()* on the homicides for each location in the dataset

3. Calculated the OR and 95% CI for victim race for each city

4. Created **city_state_order** variable to record the order of the OR's of
all the cities

5. Plotted the OR's of a homicide being resolved for a white vs. non-white 
victim after adjusting for age and sex in order of increasing magnitude for
all cities. 
```{r}
logistic_regression <- function(df) {
  broom::tidy(glm(solved_binary ~ victim_age + victim_sex + victim_race, 
                  data = df, family = binomial()))
}

regression_df <- homicide_df %>% 
  group_by(city_state) %>% 
  nest %>% 
  mutate(log_results = map(data, logistic_regression)) %>% 
  select(city_state, log_results) %>% 
  unnest %>%
  filter(term == "victim_racewhite") %>% 
  mutate(OR = exp(estimate),
         CI_lower_limit = exp(estimate - 1.96*std.error),
         CI_upper_limit = exp(estimate + 1.96*std.error)) %>% 
  select(city_state, estimate, OR, CI_lower_limit, CI_upper_limit)

regression_df

city_state_order <- regression_df$city_state[order(regression_df$OR, 
                                                   decreasing = T)]

regression_df %>% 
  mutate(city_state = readr::parse_factor(city_state, city_state_order)) %>% 
  ggplot(aes(x = city_state, y = OR)) + 
  geom_point() +
  geom_errorbar(aes(ymin = CI_lower_limit, ymax = CI_upper_limit), 
                alpha = 0.5) +
  geom_hline(yintercept = 1, linetype = "longdash", alpha = 0.75) +
  theme(axis.text.y = element_text(size = 4.5)) +
  labs(title = "Comparing the odds of a homicide being resolved for a 
       white vs. non-white victim after adjusting for age and sex",
       x = "Location",
       y = "Odds ratio with 95% confidence intervals") +
  scale_y_continuous(breaks = c(1, 5, 10, 15, 20),
                     labels = c('1', '5', '10', '15', '20')) +
  coord_flip()
```

In all cities except Tampa, Birmingham, and Durham, after accounting for the 
victim's age and sex, a homicide with a white victim has greater odds of being 
resolved compared to a non-white victim (without considering confidence 
intervals). There are a couple of outlier cities such as Boston and Oakland in 
which the odds of a homicide with a white victim being resolved are more than 2 
times the odds of a homicide with a non-white victim being resolved. For several 
cities, the 95% confidence interval does not overlap with the null value of 1, 
which suggests the disparity between homicides with white vs. non-white victims
is statistically significant. 


# Problem 2

### Loading and cleaning the data

1. Load the dataset

2. Transform the variables for the baby's sex, father's and mother's race, 
and presence of malformation into factors

3. Check if any variables have missing values
```{r}
birthweight_df <- readr::read_csv('./data/birthweight.csv') %>% 
  mutate(babysex = recode_factor(babysex, `1` = 'Male', `2` = 'Female'),
         frace = recode_factor(frace, `1` = 'White', `2` = 'Black', 
                               `3` = 'Asian', `4` = 'Puerto Rican', 
                               `8` = 'Other', `9` = 'Unknown'),
         malform = recode_factor(malform, `0` = 'Absent', `1` = 'Present'),
         mrace = recode_factor(mrace, `1` = 'White', `2` = 'Black',
                               `3` = 'Asian', `4` = 'Puerto Rican',
                               `8` = 'Other'))

map(birthweight_df, ~sum(is.na(.)))
```

### Proposing a regression model for birthweight

I propose a regression model which models birthweight by family income, 
gestational age, presence of malformations, average number of cigarettes smoked
per day during pregnancy, and mother's weight gain during pregnancy. These 
are factors which I believe would influence the birthweight of a baby. 

1. Run a linear regression for my proposed model

2. Plot the residuals by predicted values of my proposed model
```{r}
my_model <- lm(data = birthweight_df, bwt ~ fincome + gaweeks + malform + 
                 smoken + wtgain)

birthweight_df %>% 
  add_predictions(my_model) %>% 
  add_residuals(my_model) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = 0.25)
```

The majority of predicted values for my model lie within 500 grams of the 
actual value. There are a handful of outliers with large residuals for
the smallest predicted values. 


### Testing my proposed model against the other 2 models
1. Write functions for running linear regressions according to the specified 
model (my proposed model, main effects only, and all effects)

2. Write a function for calculating the root mean square error (RMSE) of each
cross-validation for each of the three models

3. Make 100 different training and testing datasets based on the original data,
and run all 3 models on those 100 sets of training and testing sets. Then 
compute the RMSE for every fitting of a model onto a set of training and testing
data.

4. Graph the distribution of RMSE's by the specified model.
```{r}
my_model_regression <- function(df) {
  lm(data = df, bwt ~ fincome + gaweeks + malform + smoken + wtgain)
}

main_fx_regression <- function(df) {
  lm(data = df, bwt ~ blength + gaweeks)
}

all_fx_regression <- function(df) {
  lm(data = birthweight_df, bwt ~ bhead + blength + babysex +
                     bhead*blength + bhead*babysex + blength*babysex +
                     bhead*blength*babysex)
}

rmse_function <- function(df1, df2) {
  rmse(model = df1, data = df2)
}

cv_df <- crossv_mc(birthweight_df, 100) %>% 
  mutate(train = map(train, as_tibble),
         test = map(test, as_tibble)) %>% 
  mutate(my_mod = map(train, my_model_regression),
         main_fx_mod = map(train, main_fx_regression),
         all_fx_mod = map(train, all_fx_regression)) %>% 
  mutate(rmse_my_mod = map2_dbl(my_mod, test, rmse_function),
         rmse_main_fx = map2_dbl(main_fx_mod, test, rmse_function),
         rmse_all_fx = map2_dbl(all_fx_mod, test, rmse_function))

cv_df %>% 
  select(starts_with("rmse")) %>% 
  gather(key = model, value = rmse) %>% 
  mutate(model = str_replace(model, "rmse_", ""),
         model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

Comparing the distribution of RMSE's for the three models run over 100 different
corss-validations, my proposed model (my_mod) has the largest average RMSE of 
the 3 models. The main effects only model (main_fx) performs better than my_mod,
but the RMSE's do not appear to be normally distributed. The model with all 
effects (all_fx) has the smallest average RMSE, and the RMSE's appear to be
normally distributed. 
